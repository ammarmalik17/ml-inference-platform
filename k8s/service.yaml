apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  labels:
    app: ml-inference-service
spec:
  selector:
    app: ml-inference-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
      name: http-api
  type: LoadBalancer
  # Optional: Uncomment and configure based on cloud provider requirements
  # annotations:
  #   service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # Use Network Load Balancer for low latency