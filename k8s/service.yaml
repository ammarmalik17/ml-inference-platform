apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  labels:
    app: ml-inference-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # Use Network Load Balancer for low latency
spec:
  selector:
    app: ml-inference-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer